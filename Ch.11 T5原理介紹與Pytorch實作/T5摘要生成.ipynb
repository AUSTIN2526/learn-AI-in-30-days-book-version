{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c831d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SentencePiece in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.1.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from rouge) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install SentencePiece\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e446048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch.optim as optim \n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e917e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed) \n",
    "    np.random.seed(seed)  \n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seeds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6827ca",
   "metadata": {},
   "source": [
    "## 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8ec9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:173: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid= [], [], [], []\n",
    "for types in os.listdir('SummaryData'):\n",
    "    classes_path = f'SummaryData/{types}'\n",
    "    for classes in os.listdir(classes_path):\n",
    "        file_path = f'{classes_path}/{classes}'\n",
    "        df = pd.read_csv(file_path).values\n",
    "        input_text = [\"summarize: \" + i for i in df[:,1]]\n",
    "        summary = df[:,0]\n",
    "        if types == 'Train':\n",
    "            x_train.extend(input_text)\n",
    "            y_train.extend(summary)\n",
    "        \n",
    "        else:\n",
    "            x_valid.extend(input_text)\n",
    "            y_valid.extend(summary)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "PAD_IDX = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73991ec5",
   "metadata": {},
   "source": [
    "## 創建資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543ce9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class News(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "          \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "trainset = News(x_train, y_train)\n",
    "validset = News(x_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b90f8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):   \n",
    "    (x, y) = zip(*batch)\n",
    "    tokens = tokenizer(x, truncation=True, max_length=128, padding=\"longest\", return_tensors='pt')\n",
    "    input_ids = tokens.input_ids\n",
    "    attention_mask = tokens.attention_mask\n",
    "    labels = tokenizer(y, truncation=True, max_length=128, padding=\"longest\", return_tensors='pt').input_ids\n",
    "    labels[labels == PAD_IDX] = -100\n",
    "    return input_ids, attention_mask, labels\n",
    "      \n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size = 8, shuffle = True, num_workers = 0, pin_memory = True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(validset, batch_size = 8, shuffle = True, num_workers = 0, pin_memory = True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e134f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7b2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    train_pbar = tqdm(train_loader, position=0, leave=True) # 宣告進度條\n",
    "    \n",
    "    model.train() # 將模型切換成訓練模式\n",
    "    for input_datas in train_pbar: \n",
    "        features, maskes, labels = [i.to(device) for i in input_datas] # 將資料放入到GPU中\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        outputs = model(input_ids =features , attention_mask = maskes, labels = labels) # 模型計算答案(前向傳播)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward() # 返向傳播\n",
    "        optimizer.step() # 更新模型權重\n",
    "        \n",
    "        train_pbar.set_description(f'Train Epoch {epoch}')  # 顯示訓練次數\n",
    "        train_pbar.set_postfix({'loss':f'{loss:.3f}'}) # 顯示當下模型損失\n",
    "\n",
    "        train_loss += loss.item()  # 模型總損失\n",
    "    return train_loss/len(train_loader)\n",
    "\n",
    "def valid(epoch):\n",
    "    valid_loss = 0\n",
    "    valid_pbar = tqdm(valid_loader, position=0, leave=True)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for input_datas in valid_pbar:\n",
    "            features, maskes, labels = [i.to(device) for i in input_datas] # 將資料放入到GPU中\n",
    "        \n",
    "            outputs = model(input_ids = features , attention_mask = maskes, labels = labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            valid_pbar.set_description(f'Valid Epoch {epoch}')\n",
    "            valid_pbar.set_postfix({'loss':f'{loss:.3f}'})\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss/len(valid_loader)\n",
    "\n",
    "        \n",
    "def show_training_loss(loss_record):\n",
    "    train_loss, valid_loss = [i for i in loss_record.values()]\n",
    "    \n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(valid_loss)\n",
    "    #標題\n",
    "    plt.title('Result')\n",
    "    #y軸標籤\n",
    "    plt.ylabel('Loss')\n",
    "    #x軸標籤\n",
    "    plt.xlabel('Epoch')\n",
    "    #顯示折線的名稱\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    #顯示折線圖\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd3ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: 100%|██████████| 11468/11468 [1:05:33<00:00,  2.92it/s, loss=1.006]\n",
      "Valid Epoch 0: 100%|██████████| 14222/14222 [17:37<00:00, 13.45it/s, loss=1.766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model With Loss 2.25595\n",
      "Train Loss: 2.65385 Valid Loss: 2.25595| Best Loss: 2.25595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 11468/11468 [1:04:52<00:00,  2.95it/s, loss=1.126]\n",
      "Valid Epoch 1: 100%|██████████| 14222/14222 [17:36<00:00, 13.47it/s, loss=2.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model With Loss 2.03159\n",
      "Train Loss: 1.67018 Valid Loss: 2.03159| Best Loss: 2.03159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 11468/11468 [1:04:53<00:00,  2.95it/s, loss=0.968]\n",
      "Valid Epoch 2: 100%|██████████| 14222/14222 [17:35<00:00, 13.47it/s, loss=2.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.36647 Valid Loss: 2.04746| Best Loss: 2.03159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 11468/11468 [1:05:00<00:00,  2.94it/s, loss=1.040]\n",
      "Valid Epoch 3: 100%|██████████| 14222/14222 [17:07<00:00, 13.84it/s, loss=1.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.11982 Valid Loss: 2.04645| Best Loss: 2.03159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 11468/11468 [1:03:00<00:00,  3.03it/s, loss=0.777]\n",
      "Valid Epoch 4: 100%|██████████| 14222/14222 [17:07<00:00, 13.84it/s, loss=1.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model With Loss 2.02005\n",
      "Train Loss: 0.92086 Valid Loss: 2.02005| Best Loss: 2.02005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|██████████| 11468/11468 [1:04:31<00:00,  2.96it/s, loss=0.503]\n",
      "Valid Epoch 5: 100%|██████████| 14222/14222 [17:33<00:00, 13.50it/s, loss=2.463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.77247 Valid Loss: 2.13717| Best Loss: 2.02005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|██████████| 11468/11468 [1:02:55<00:00,  3.04it/s, loss=0.945]\n",
      "Valid Epoch 6: 100%|██████████| 14222/14222 [17:04<00:00, 13.89it/s, loss=2.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.65550 Valid Loss: 2.19584| Best Loss: 2.02005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|██████████| 11468/11468 [1:02:48<00:00,  3.04it/s, loss=0.847]\n",
      "Valid Epoch 7: 100%|██████████| 14222/14222 [17:03<00:00, 13.89it/s, loss=1.968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "|Model can't improve, stop training|\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVBElEQVR4nO3dd3wUdf4/8Ndsyqb3XikJhBqSQCAUCb0o0hQF7gA9vZ8KCnp4HuepoKfo91SwIIgKnCeIShNF6RA6AUKQ3lNJJcmmbza78/tjk4VAWJKw2dnyej4e8yA7Mzv73qXsi898iiCKoggiIiIiCyGTugAiIiIiQ2K4ISIiIovCcENEREQWheGGiIiILArDDREREVkUhhsiIiKyKAw3REREZFEYboiIiMiiMNwQERGRRWG4ISK6jSAImD9/vtRlENEDYLghIqNatWoVBEHQbba2tggODsaMGTOQnZ0tdXl3OXToEObPn4+SkhKpSyGiJrKVugAisk5vv/022rZti+rqahw5cgSrVq3CgQMHcObMGTg4OEhdns6hQ4ewYMECzJgxAx4eHlKXQ0RNwHBDRJIYNWoUevbsCQB45pln4OPjgw8++ACbN2/GpEmTJK6OiMwZb0sRkUkYMGAAAODq1au6fRcuXMBjjz0GLy8vODg4oGfPnti8eXOD56lUKixYsACRkZFwcHCAt7c3+vfvjx07dujOSUxMRGJi4l2vOWPGDLRp0+aeNc2fPx+vvvoqAKBt27a6W2lpaWktf6NE1OrYckNEJqE+MHh6egIAzp49i379+iE4OBj/+Mc/4OzsjB9//BHjxo3D+vXrMX78eADaALJw4UI888wziI+PR2lpKY4fP46UlBQMGzbsgWqaMGECLl26hO+//x6LFi2Cj48PAMDX1/eBrktErYvhhogkoVAoUFhYiOrqahw9ehQLFiyAXC7HI488AgCYPXs2wsLCcOzYMcjlcgDACy+8gP79++O1117ThZstW7Zg9OjRWL58ucFr7N69O2JjY/H9999j3Lhxelt5iMh08LYUEUli6NCh8PX1RWhoKB577DE4Oztj8+bNCAkJQVFREXbv3o1JkyahrKwMhYWFKCwsxM2bNzFixAhcvnxZN7LKw8MDZ8+exeXLlyV+R0RkKhhuiEgSS5YswY4dO7Bu3TqMHj0ahYWFuhaaK1euQBRFvPHGG/D19W2wvfXWWwCA/Px8ANpRVyUlJejQoQO6deuGV199FX/88Ydk74uIpMfbUkQkifj4eN1oqXHjxqF///6YMmUKLl68CI1GAwCYO3cuRowY0ejzIyIiAAAPPfQQrl69ip9//hnbt2/H119/jUWLFmHZsmV45plnAGgn5hNF8a5rqNXq1nhrRCQxhhsikpyNjQ0WLlyIQYMG4fPPP8fTTz8NALCzs8PQoUPv+3wvLy889dRTeOqpp1BeXo6HHnoI8+fP14UbT09PXLt27a7npaen3/fagiA0890QkdR4W4qITEJiYiLi4+OxePFiuLm5ITExEV9++SVycnLuOregoED3882bNxscc3FxQUREBJRKpW5f+/btceHChQbPO3XqFA4ePHjfupydnQGAMxQTmRG23BCRyXj11Vfx+OOPY9WqVViyZAn69++Pbt264dlnn0W7du2Ql5eHw4cPIysrC6dOnQIAdO7cGYmJiYiLi4OXlxeOHz+OdevWYdasWbrrPv300/j4448xYsQI/OUvf0F+fj6WLVuGLl26oLS0VG9NcXFxAIDXX38dTz75JOzs7DBmzBhd6CEiEyQSERnRypUrRQDisWPH7jqmVqvF9u3bi+3btxdra2vFq1evitOmTRMDAgJEOzs7MTg4WHzkkUfEdevW6Z7z73//W4yPjxc9PDxER0dHMSoqSnz33XfFmpqaBtf+7rvvxHbt2on29vZijx49xG3btonTp08Xw8PDG5wHQHzrrbca7HvnnXfE4OBgUSaTiQDE69evG+rjIKJWIIhiI73siIiIiMwU+9wQERGRRWG4ISIiIovCcENEREQWheGGiIiILArDDREREVkUhhsiIiKyKFY3iZ9Go8GNGzfg6urKadWJiIjMhCiKKCsrQ1BQEGQy/W0zVhdubty4gdDQUKnLICIiohbIzMxESEiI3nOsLty4uroC0H44bm5uEldDRERETVFaWorQ0FDd97g+Vhdu6m9Fubm5MdwQERGZmaZ0KWGHYiIiIrIoDDdERERkURhuiIiIyKJYXZ+bplKr1VCpVFKXYbbs7OxgY2MjdRlERGSFGG7uIIoicnNzUVJSInUpZs/DwwMBAQGcT4iIiIyK4eYO9cHGz88PTk5O/GJuAVEUUVlZifz8fABAYGCgxBUREZE1Ybi5jVqt1gUbb29vqcsxa46OjgCA/Px8+Pn58RYVEREZDTsU36a+j42Tk5PElViG+s+RfZeIiMiYGG4awVtRhsHPkYiIpMBwQ0RERBaF4Ybu0qZNGyxevFjqMoiIiFqEHYotRGJiInr06GGQUHLs2DE4Ozs/eFFEREQSYLgxIJVag1q1CEd70xsZJIoi1Go1bG3v/1vu6+trhIqIiIhaB29LGYiisgYXcsqQXVJl9NeeMWMGkpKS8Mknn0AQBAiCgFWrVkEQBPz++++Ii4uDXC7HgQMHcPXqVYwdOxb+/v5wcXFBr169sHPnzgbXu/O2lCAI+PrrrzF+/Hg4OTkhMjISmzdvNvK7JCIiahqGm/sQRRGVNbX33SAA1So1iiqUuFmubNJz7reJotikGj/55BMkJCTg2WefRU5ODnJychAaGgoA+Mc//oH3338f58+fR/fu3VFeXo7Ro0dj165dOHnyJEaOHIkxY8YgIyND72ssWLAAkyZNwh9//IHRo0dj6tSpKCoqeuDPl4iIyNB4W+o+qlRqdH5zmySvfe7tEXCyv/9vkbu7O+zt7eHk5ISAgAAAwIULFwAAb7/9NoYNG6Y718vLC9HR0brH77zzDjZu3IjNmzdj1qxZ93yNGTNmYPLkyQCA9957D59++imSk5MxcuTIFr03IiKi1sKWGwvXs2fPBo/Ly8sxd+5cdOrUCR4eHnBxccH58+fv23LTvXt33c/Ozs5wc3PTLa9ARERkSthycx+OdjY49/aIJp0riiKu5FdAWatGkLsjvFzsH/i1H9Sdo57mzp2LHTt24MMPP0RERAQcHR3x2GOPoaamRu917OzsGjwWBAEajeaB6yMiIjI0hpv7EAShSbeG6gV5OCJHUYVKlRrBdjZGm6XX3t4earX6vucdPHgQM2bMwPjx4wFoW3LS0tJauToiIiLj4W0pA/N0soMgCKhWqVFVc/+wYSht2rTB0aNHkZaWhsLCwnu2qkRGRmLDhg1ITU3FqVOnMGXKFLbAEBGRRWG4MTBbGxk8HLW3cG5W6L/VY0hz586FjY0NOnfuDF9f33v2ofn444/h6emJvn37YsyYMRgxYgRiY2ONVicREVFrE8Smjje2EKWlpXB3d4dCoYCbm1uDY9XV1bh+/Tratm0LBweHFr9GhbIWVwvKIRMERAW4wtbGOjOkoT5PIiIifd/fd7LOb91W5mRvAwc7G2hEEcWVKqnLISIisioMN61AEAR4O2tHShVV1DR5Mj4iIiJ6cAw3rcTDyR4yQYCyVo0KpfE6FhMREVk7hptWYiMT4OGk7VhcVKGUuBoiIiLrwXDTiryd5QAARXUtVGoOtyYiIjIGhptW5GhvAyd7W4iiiGIjDgsnIiKyZgw3rcyLHYuJiIiMiuGmlXk42sFGJqBGrUGZslbqcoiIiCwew00rk8kEeDrVtd6U89YUERFRa2O4MYL6W1Nl1SrU1Jpmx+I2bdpg8eLFuseCIGDTpk33PD8tLQ2CICA1NbXVayMiImoOrgpuBA52NnCR26JcWYuiihoEuJv+UgQ5OTnw9PSUugwiIqJmY8uNkdS33hRX1kBjBh2LAwICIJfLpS6DiIio2SQNNwsXLkSvXr3g6uoKPz8/jBs3DhcvXtT7nFWrVkEQhAabOSzK6OZoB1uZDCq1BmXVhl1vavny5QgKCoJG0/CW19ixY/H000/j6tWrGDt2LPz9/eHi4oJevXph586deq95522p5ORkxMTEwMHBAT179sTJkycN+h6IiIgMRdJwk5SUhJkzZ+LIkSPYsWMHVCoVhg8fjoqKCr3Pc3NzQ05Ojm5LT09vvSJFEaipeOBNpqqEt30NBFUliopLmva8JrbwPP7447h58yb27Nmj21dUVIStW7di6tSpKC8vx+jRo7Fr1y6cPHkSI0eOxJgxY5CRkdGk65eXl+ORRx5B586dceLECcyfPx9z585tyadJRETU6iTtc7N169YGj1etWgU/Pz+cOHECDz300D2fJwgCAgICWrs8LVUl8F6QQS7lX7c12T9vAPbO9z3N09MTo0aNwpo1azBkyBAAwLp16+Dj44NBgwZBJpMhOjpad/4777yDjRs3YvPmzZg1a9Z9r79mzRpoNBp88803cHBwQJcuXZCVlYXnn3++Oe+GiIjIKEyqz41CoQAAeHl56T2vvLwc4eHhCA0NxdixY3H27FljlGfSpk6divXr10Op1K5jtXr1ajz55JOQyWQoLy/H3Llz0alTJ3h4eMDFxQXnz59vcsvN+fPn0b179wa3/xISElrlfRARET0okxktpdFoMGfOHPTr1w9du3a953kdO3bEihUr0L17dygUCnz44Yfo27cvzp49i5CQkLvOVyqVui98ACgtLW1eYXZO2hYUAymtUiG9qBK2MgEd/V0hkwn6X7uJxowZA1EUsWXLFvTq1Qv79+/HokWLAABz587Fjh078OGHHyIiIgKOjo547LHHUFPDeXeIiMjymEy4mTlzJs6cOYMDBw7oPS8hIaFBq0Hfvn3RqVMnfPnll3jnnXfuOn/hwoVYsGBBywsThCbdGmoqVzsRtpXajsUKjT08HewNcl0HBwdMmDABq1evxpUrV9CxY0fExsYCAA4ePIgZM2Zg/PjxALQtX2lpaU2+dqdOnfC///0P1dXVutabI0eOGKRuIiIiQzOJ21KzZs3Cr7/+ij179jTa+qKPnZ0dYmJicOXKlUaPz5s3DwqFQrdlZmYaouQWEwTh1npTBp6xeOrUqdiyZQtWrFiBqVOn6vZHRkZiw4YNSE1NxalTpzBlypS7RlbpM2XKFAiCgGeffRbnzp3Db7/9hg8//NCgtRMRERmKpOFGFEXMmjULGzduxO7du9G2bdtmX0OtVuP06dMIDAxs9LhcLoebm1uDTWpezvYQIKCiphZVKrXBrjt48GB4eXnh4sWLmDJlim7/xx9/DE9PT/Tt2xdjxozBiBEjdK06TeHi4oJffvkFp0+fRkxMDF5//XV88MEHBqubiIjIkARRwqWqX3jhBaxZswY///wzOnbsqNvv7u4OR0dHAMC0adMQHByMhQsXAgDefvtt9OnTBxERESgpKcF//vMfbNq0CSdOnEDnzp3v+5qlpaVwd3eHQqG4K+hUV1fj+vXraNu2bavPnZN+swKKKhW8XeQI9nBs1deSijE/TyIismz6vr/vJGmfm6VLlwIAEhMTG+xfuXIlZsyYAQDIyMiATHargam4uBjPPvsscnNz4enpibi4OBw6dKhJwcaUeDnbQ1GlQklFDQLcHGCjr2MxERERNZmk4aYpjUZ79+5t8HjRokW6UUDmzEVuC7mtDMpaDRRVNfBy5lIHREREhmASHYqtkbZjsTbQ3CyvaVLQIyIiovtjuJGQp5MdBEFAlUpt0I7FRERE1ozhphHGakWxtZHBw9EOgOGHhZsCtkYREZEUGG5uY2enDRqVlZVGe836OW9KqlSobcbcM+ag/nOs/1yJiIiMwWRmKDYFNjY28PDwQH5+PgDAyckJgtC6o5hkogg7qFGjUqOgWICns2FmLJaSKIqorKxEfn4+PDw8YGNjI3VJRERkRRhu7lC/2nh9wDGGcmUtSipVKLYR4O9mOfPBeHh4GG/1diIiojoMN3cQBAGBgYHw8/ODSqUyymtWKFWY9OURVKvU+HhSNKJDPY3yuq3Jzs6OLTZERCQJhpt7sLGxMdqXs4ODA+Ij/PF9ciZWH89F78jGl5IgIiKi+2OHYhMxJT4cAPD7mRwUlislroaIiMh8MdyYiG4h7ogOcYdKLeKn41lSl0NERGS2GG5MyNQ+2tabNcnp0Gg4RwwREVFLMNyYkDHdg+DqYIvMoirsv1IodTlERERmieHGhDja22BibAgAYPWRdImrISIiMk8MNyZmau8wAMCuC/nIUVRJXA0REZH5YbgxMZH+rohv6wW1RsTa5EypyyEiIjI7DDcm6E91HYvXHstArdqy1psiIiJqbQw3JmhEF394O9sjr1SJXReMtwwEERGRJWC4MUFyWxs83jMUALD6aIbE1RAREZkXhhsTNSVe27F436UCZNyslLgaIiIi88FwY6LCvJ3wUAdfAMDqZA4LJyIiaiqGGxP2p7ph4T8dz4KyVi1xNUREROaB4caEDY7yQ4CbA4oqarD1TK7U5RAREZkFhhsTZmsjw5Px7FhMRETUHAw3Ju7JXmGwkQlIvl6ES3llUpdDRERk8hhuTFyAuwOGRPkBANaw9YaIiOi+GG7MQP2MxetTslBZUytxNURERKaN4cYM9I/wQZiXE8qqa/HrqRypyyEiIjJpDDdmQCYTMKVuWPjqo5zzhoiISB+GGzPxeFwI7G1kOJWlwOkshdTlEBERmSyGGzPh7SLHqG4BANh6Q0REpA/DjRmZ2lvbsfjn1BsorVZJXA0REZFpYrgxpOv7gNqaVrt8rzaeiPRzQZVKjU0ns1vtdYiIiMwZw42hFF4G/jceWBIPnPsZEEWDv4QgCJha37H4SAbEVngNIiIic8dwYyiKLMDJGyi+Dvw4DVg5Csg+YfCXGR8bAkc7G1zMK8Px9GKDX5+IiMjcMdwYSvtBwIspwMDXAFtHIOMw8NVgYP2zQEmmwV7G3dEOj0YHAQBWH2HHYiIiojsx3BiS3AUY9E/gxRNA9BTtvtM/Ap/3BHa9DSgNszbU1D7aW1O/nc5FUUXr9fEhIiIyRww3rcE9GBi/FPhrEtBmAFBbDez/CPg0Bji+ElA/2BIK3UM80C3YHTVqDdadMFyrEBERkSVguGlNQT2A6b8AT64BvNoDFQXAr3OAZf2BKzsf6NK6jsVHM6DRsGMxERFRPYab1iYIQNTDwAtHgJEfAI6eQMF54LuJwP8mAHnnWnTZR3sEwVVui/SblTh4tdDARRMREZkvhhtjsbUH+jwHvHQSSJgFyOyAq7uAZf2AX2YD5fnNupyTvS0mxAYD0A4LJyIiIi2GG2Nz9ARGvAvMSgY6jwVEDXBilbY/zr4PAVVVky81pW7G4h3n85BXWt1KBRMREZkXQbSymeBKS0vh7u4OhUIBNzc3qcsB0g8D2/4J3EjRPnYLAYa8CXR7HJDdP3s+vuwQjqUV45VhHfDSkMhWLpaIiBpVUwEUXQNuXgHKcut2CoAg03ZPEIS6x3X7GhyTNXLs9ufceZ6skWOCnmONXV8GCNBzrLFr4D7Xv+2YjR3g6GHQj7g5398MN6ZAowHOrAd2LQAUdaOfgmKAEe8B4X31PnXTyWzM+SEVge4O2P/3QbC1YWMcEVGrqFUCxWnAzavaEFN0te7nq0DZDamrMy0hvYBnHmzgzJ2a8/1ta9BXppaRyYDujwOdHgGOfAHsXwTcOKmd5bjTGGDoAsC7faNPHdk1AJ6/2CFHUY09FwswrLO/kYsnIrIgGjVQkqENLEV1IaY+zCgytV0J7sXRC/COANxDtK0YogaAqP1VFOt+rt8aO6a545io51hjz7v9NTR6rqm5Ry36ro8m1lz/+QiG/71pBrbcmKLyfGDPe0DKf7V/UGR2QPxfgYGvavvs3GHhb+fx5b5rSOzoi1VPxUtQMBGRGRFFoPTGHeGlLswUXQc0qns/194V8G6nDTFe7bW/ercHvNoBTl7Gew+mThTrblEZDm9L6WEW4aZe/nlg+xvAlR3axw4eQOI/gJ5/0Y6+qpNWWIHED/dCEIB9rw5CqJeTNPUSEZkKUQQqbzZseam/jVR0DVBV3vu5NvJbgaU+vNSHGRc/g39pU9Mw3OhhVuGm3pVdwPZ/Afl1c+J4tQOGvaOdP6fuL9mfvzmK/ZcL8UJie/x9ZJSExRIRGVG1omHLy+0tMUrFvZ8nswU8wm8LL+1vtcS4BTdpQAcZF8ONHmYZbgDtfeCT/wN2vwtU1M2JE95PO6w8KAZbz+Tgue9S4ONij0P/GAJ7W/7FJCILUVOpbW3RhZdrt1piKgr0PFHQ9n+5PbjUhxmPMO2IHjIb7FBsiWQ2QNwMoOtE4MBi4PDnQPpBYHki0P1JDEl8HX6ucuSXKbHtbC7G1K0cTkRkFmprgJL0xm8jlWbrf66Lf114ad/wFpJXW8DO0Tj1k0lhy425UmRpVxr/4wftY1sHHAmYgr9c6Ydu7YKx9q8J0tZHRHQnjVo74qix20glGYCovvdzHTzu6P/S7tavDmb8bzk1GW9L6WEx4aZedgqw7XUg4xAAoEB0x4e1k/Dsi28gIsBd4uKIyOqIonYSuzv7v9y8AhRfB9Q1936unXPjI5G8IzgSiRhu9LG4cANo/zG58Cuw403tfWkAeY7t4f/Yh0D7wRIXR0QWr6pYO/Dh8nbgyk7tKKV7sbEHPNs27MhbH2ZcAzgSie6J4UYPiww39WprcHnLIvimfAIPoUK7L2IYMPzfgB9HUBGRgYiidvTm5e3Ape1A5tGGt5QEWd1IpNuCS32QcQ/V9iEkaiZ2KLZWtvZoP+bvePh8FB4r/x5P2e2A7MoO4OpuIG46kPhPwMVX6iqJyBzVVALX9wGXtwGXd9xaKqaebycgchjQYQQQEt9gLi4iY2O4sTAymYAxfbrgna1/xnHfCVjq/7P2ltXxFcAfPwEDXgH6vADYOUhdKhGZuuI0bZC5tA1I2w/UVt86ZusAtH0IiByu3TzDJSuT6E68LWWBCsqU6Pv+LqjUIn59sT+6qs5oVx7PSdWe4B4KDJ2vHVbO+9tEVE+tAjKOaFtnLm0HCi82PO4eqg0yHUYAbQYA9pwNnYyHfW70sIZwAwCz1qTg1z9yMDk+DAsndNOuPH76J+3K4/VzRgTHaVceD+sjbbFEJJ3yAu0SL5e2AVf3NJzVV7DR/vtQH2h8o/gfIpIMw40e1hJuDl+9iclfHYGTvQ2O/nMIXB3qZuKsqQSOLNFOBFhTrt3Xeax25XGvtpLVS0RGotFoW3Evb9du2SnQLvlcx8lbOxChw3DtaMtGFuslkgLDjR7WEm5EUcTQj5NwtaAC74zrij/3ueN+eFkesOdd7ZIOokY7PDP+r8BDrwKOHpLUTEStpLoUuLZHe6vpyg6gPK/h8cBoIHKEtoUmOJajmcgkMdzoYS3hBgBWHLiOt389h6gAV/w+ewCExpqT885qF+W8ulv72NGrbuXxp7nuCpG5EkWg8HJd68w2IP0woFHdOm7vArRL1N5qihgGuAVKVipRUzHc6GFN4UZRqUL8ezuhrNVg/fN9EReup3n58k5g++tAwQXtY+8I7crjHUfxHjuROVBVA+kHtK0zl7dpRzrdzjuirnVmGBDeF7CVS1ImUUtxnhsCALg72WFMdBDWncjC6iPp+sNN5FDt/+ROfqtdefzmFWDtZO2IiOH/BoJ6GKtsImoqRfatvjPX9gKqylvHbOyB8H7a1pnI4doJ9IisBFtuLNzJjGKM/+IQ7G1lODpvCDydmzCxVnUpcGARcHgJoFYCEIDoycCQNwA3rjZOJBmNGsg6ph3ZdHk7kHem4XHXQG3LTOQI7X9W5C6SlEnUGnhbSg9rCzeiKOKRzw7g7I1S/OvhTnhmQLumP7kkQ7vy+OmftI9tHYF+LwF9X+I/mkTGUllUt27TNu26TVXFtx0UgJBe2pFNkSOAgG68jUwWi+FGD2sLNwCw5mgG/rnxNNr6OGP33wY23rFYn6wT2kkAM49oH7sEAIP/BfSYwlEVRIYmitoWmfp1m7KStSMa6zm4AxFDtWEmYijg7C1drURGxHCjhzWGm3JlLfq8twvlylqseaY3+kb4NP8iogic36xdeby+o6J/V2DEu9rmb0ujUQOqqrqtou7Xytv21f1cU3H3Pt2cIcJt/4sWGv6PutH9d/ysO6+lP995nfu8Xot/vv09CdqOqg4e2i9hRw/tz44e2hE6bFVoXE0FcC3p1rpN9RNt1vPrUtc6M1y7bpMNu0uS9WGHYmrARW6LcTFB+O5IBr47mt6ycCMI2sn+OowEkr8C9v2f9n+X39btG/YO4NvB8MU3Rl3bxMDRyDFVI2FEVamd3PD2fWqlcd6LNZHZagNPfdhp8HMjv94ejuRugEwmUeGtpOjabes2HWj4Z87WEWg38Na6TR6h0tVJZIbYcmMlzueUYtQn+2ErE3DoH4Ph5/aAC2dWFgFJHwDHvgY0tdpp2ns+pZ0E0FZumMChqqoLHXc85/b5OozBzgmwc7zj1/qfb9tv76x974INAFHb2gU08Wc03K97bOifdS/WzPqa+DOgHZJcXQJUlQDVCu3P6ho8EEGmDTiNBZ9Gf60PTp7an03h9mltDZBxuO520zbg5uWGxz3Cb41satNf+2eKiHR4W0oPaw03ADBx6SGcSC/G3OEdMGtwpGEuWnhFe6vq4hbDXK9ZBG2g0IUM5ztCyB3B4859DUKKU+Pn2zrwVsqDEkVtMNUFnpKGwafBvtt+rVZof66tevAa5G51YcddTzjybPzYg0xmWZbXcN2mmrJbx2S2QFjCrXWbfDrwzxqRHrwtRY2a2jsMJ9KL8X1yJp5PjICNzAD/kPpEAJPXANf3a2c6rl95XLC5R/C4I0jY3yNc6A0edcds7PllYA4EQft7Zu/UsqkEapX3CED6wlHdsfr105Sl2k1x9+Xvy865eS1Goqhd6uDyduDGyYbXcvatu9U0TLtuk4N7CwoiovuRtOVm4cKF2LBhAy5cuABHR0f07dsXH3zwATp27Kj3eT/99BPeeOMNpKWlITIyEh988AFGjx7dpNe05pabapUafRbuQkmlCt9M74khnfwN/yLKMm1rB5duIFOgVt1qAWrQKlRynxYjRcPVsR9EUIx2ZFOH4UBgjOX1HSIyErNpuUlKSsLMmTPRq1cv1NbW4p///CeGDx+Oc+fOwdnZudHnHDp0CJMnT8bChQvxyCOPYM2aNRg3bhxSUlLQtWtXI78D8+JgZ4PHYkPw9YHrWH00o3XCjdzV8NckaikbO8DZR7s1l0Z9j9ah+7QY1VbXzT1Tt26Tayv8PSMivUyqz01BQQH8/PyQlJSEhx56qNFznnjiCVRUVODXX3/V7evTpw969OiBZcuW3fc1rLnlBgCuFZRj8EdJEARg/98HIcTTSeqSiIiI7qs5398m1T6qUGibgb28vO55zuHDhzF06NAG+0aMGIHDhw83er5SqURpaWmDzZq183VBvwhviCKwNjlT6nKIiIgMzmTCjUajwZw5c9CvXz+9t5dyc3Ph79+wmdff3x+5ubmNnr9w4UK4u7vrttBQzhcxtXc4AGDtsUyo1Jr7nE1ERGReTCbczJw5E2fOnMHatWsNet158+ZBoVDotsxMtlYM6+wPX1c5CsuV2H42T+pyiIiIDMokws2sWbPw66+/Ys+ePQgJCdF7bkBAAPLyGn4h5+XlISAgoNHz5XI53NzcGmzWzs5Ghid6aluwVh9Nl7gaIiIiw5I03IiiiFmzZmHjxo3YvXs32rZte9/nJCQkYNeuXQ327dixAwkJCa1VpkV6Mj4UggAcunoTVwvKpS6HiIjIYCQNNzNnzsR3332HNWvWwNXVFbm5ucjNzUVV1a0ZSadNm4Z58+bpHs+ePRtbt27FRx99hAsXLmD+/Pk4fvw4Zs2aJcVbMFshnk4Y3NEPAPD90QyJqyEiIjIcScPN0qVLoVAokJiYiMDAQN32ww8/6M7JyMhATk6O7nHfvn2xZs0aLF++HNHR0Vi3bh02bdrEOW5aYGqfMADAupQsVKvUEldDRERkGCY1z40xWPs8N7dTa0Q89H97kF1ShY8ej8bEOP39nYiIiKRitvPckHHZyARMjmfHYiIisiwMN1ZuUq9Q2MoEpGSU4NwN657gkIiILAPDjZXzc3XAiC7aYfRrktl6Q0RE5o/hhjC1t7Zj8caUbJQrayWuhoiI6MEw3BAS2nujnY8zKmrU+Dk1W+pyiIiIHgjDDUEQBEypa7357kgGrGwAHRERWRiGGwIAPBYXAntbGc7nlCI1s0TqcoiIiFqM4YYAAB5O9nikeyAAYDVnLCYiIjPGcEM6U3uHAwB+OXUDJZU1EldDRETUMgw3pBMb5oGoAFcoazVYn8KOxUREZJ4YbkhHEARM7aNtvVl9NJ0di4mIyCwx3FAD42OC4Wxvg2sFFThyrUjqcoiIiJqN4YYacJHbYmxMMACuN0VEROaJ4YbuMiVeO+fNtrO5KChTSlwNERFR8zDc0F26BrujR6gHVGoRPx7PlLocIiKiZmG4oUbVrzf1fXIG1Bp2LCYiIvPBcEONGhMdBDcHW2QVV2Hf5QKpyyEiImoyhhtqlIOdDR6LCwUArD7CGYuJiMh8MNzQPdUvprn7Qh6yS6okroaIiKhpGG7oniL8XNCnnRc0IvBDMltviIjIPDDckF71602tPZYJlVojcTVERET3x3BDeo3oEgAfF3vklymx63ye1OUQERHdF8MN6WVvK8OkntqOxd+xYzEREZkBhhu6r8nxYRAE4MCVQlwvrJC6HCIiIr0Ybui+Qr2cMLCDLwDtpH5ERESmjOGGmuRPdR2LfzqeiWqVWuJqiIiI7o3hhppkUJQfgtwdUFypwtYzuVKXQ0REdE8MN9QkNjIBT9atFv7dkXSJqyEiIro3hhtqsid6hcJGJuB4ejEu5JZKXQ4REVGjGG6oyfzdHDCskz8AYM1RdiwmIiLTxHBDzfKnPtqOxRtSslGhrJW4GiIiorsx3FCz9G3vjTbeTihX1uKXUzekLoeIiOguDDfULDKZoFst/Luj6RBFUeKKiIiIGmK4oWZ7LC4U9jYynMkuxR9ZCqnLISIiaoDhhprNy9keo7sFAABWH+WwcCIiMi0MN9Qi9R2LN5+6AUWVSuJqiIiIbmG4oRaJC/dER39XVKs02JiSJXU5REREOgw31CKCIGBqn/qOxRnsWExERCaD4YZabFxMMBztbHAlvxzJ14ukLoeIiAgAww09ADcHO4ztEQQAWM0Zi4mIyEQw3NADmdpb27H49zM5KCxXSlwNERERww09oG4h7ogOcYdKLeKn4+xYTERE0mO4oQdW33qzJjkdGg07FhMRkbQYbuiBPRIdCFcHW2QWVWH/lUKpyyEiIivXonCTmZmJrKxbtyCSk5MxZ84cLF++3GCFkflwsrfFxNgQAMDqI5yxmIiIpNWicDNlyhTs2bMHAJCbm4thw4YhOTkZr7/+Ot5++22DFkjmYWrdYpo7z+chJaNY4mqIiMiatSjcnDlzBvHx8QCAH3/8EV27dsWhQ4ewevVqrFq1ypD1kZmI9HfFQx18oRGBJ748jFUHr3NiPyIikkSLwo1KpYJcLgcA7Ny5E48++igAICoqCjk5OYarjszK51NiMLJLAFRqEfN/OYdZa06irJrrThERkXG1KNx06dIFy5Ytw/79+7Fjxw6MHDkSAHDjxg14e3sbtEAyH24Odlj6p1i8+Uhn2MoEbDmdgzGfHcC5G6VSl0ZERFakReHmgw8+wJdffonExERMnjwZ0dHRAIDNmzfrbleRdRIEAU/3b4sfn0tAkLsD0m5WYtwXB7E2metPERGRcQhiC79x1Go1SktL4enpqduXlpYGJycn+Pn5GaxAQystLYW7uzsUCgXc3NykLseiFVfU4JUfU7HnYgEAYEJMMP49viuc7G0lroyIiMxNc76/W9RyU1VVBaVSqQs26enpWLx4MS5evGjSwYaMy9PZHt9M74W/j+wImQBsOJmNsZ8fxOW8MqlLIyIiC9aicDN27Fh8++23AICSkhL07t0bH330EcaNG4elS5catEAybzKZgBcSI7Dm2T7wc5Xjcn45Hv38IDae5FINRETUOloUblJSUjBgwAAAwLp16+Dv74/09HR8++23+PTTTw1aIFmGPu28seWlAegX4Y0qlRov/3AK8zacRrVKLXVpRERkYVoUbiorK+Hq6goA2L59OyZMmACZTIY+ffogPZ0z1FLjfF3l+Pbp3nhpSCQEAfg+OQMTvjiEtMIKqUsjIiIL0qJwExERgU2bNiEzMxPbtm3D8OHDAQD5+fnspEt62cgEvDKsA/77VDy8nO1xLqcUj3x2AL+f5vxIRERkGC0KN2+++Sbmzp2LNm3aID4+HgkJCQC0rTgxMTEGLZAs00MdfPHbSwPQM9wT5cpaPL86BQt+OYuaWo3UpRERkZlr8VDw3Nxc5OTkIDo6GjKZNiMlJyfDzc0NUVFRBi3SkDgU3LSo1Bp8uP0ivky6BgCIDvXAkikxCPF0krgyIiIyJc35/m5xuKlXvzp4SEjIg1zGaBhuTNOOc3n424+pKK2uhbujHT6eFI0hnfylLouIiExEq89zo9Fo8Pbbb8Pd3R3h4eEIDw+Hh4cH3nnnHWg0vK1AzTessz+2vDQA0SHuUFSp8Jf/Hsf7v19ArZp/noiIqHlaFG5ef/11fP7553j//fdx8uRJnDx5Eu+99x4+++wzvPHGG4aukaxEqJcTfnwuATP6tgEALEu6iilfHUVeabW0hRERkVlp0W2poKAgLFu2TLcaeL2ff/4ZL7zwArKzsw1WoKHxtpR52PJHDl5b/wfKlbXwdrbHJ0/GoH+kj9RlERGRRFr9tlRRUVGjnYajoqJQVFTUkksSNfBw90BsntUPUQGuuFlRgz+vOIrFOy9BreHim0REpF+Lwk10dDQ+//zzu/Z//vnn6N69+wMXRQQA7XxdsGlmPzzZKxSiCCzeeRnTVySjsFwpdWlERGTCWnRbKikpCQ8//DDCwsJ0c9wcPnwYmZmZ+O2333RLM5gi3pYyT+tPZOFfm86gSqWGv5scn02ORXxbL6nLIiIiI2n121IDBw7EpUuXMH78eJSUlKCkpAQTJkzA2bNn8b///a9FRRPpMzEuBD/P6ocIPxfklSox+asjWJZ0FRrepiIiojs88Dw3tzt16hRiY2OhVpvuYohsuTFvFcpavL7xNDal3gAADInyw0eTouHhZC9xZURE1JpaveWGSCrOclsseqIH3hvfDfa2Muy6kI+HPz2A1MwSqUsjIiITwXBDZkcQBEzpHYYNz/dFuLcTskuq8PiyQ1h58DoM2BBJRERmStJws2/fPowZMwZBQUEQBAGbNm3Se/7evXshCMJdW25urnEKJpPSNdgdv7zYH6O6BkClFrHgl3OYuSYFpdUqqUsjIiIJ2Tbn5AkTJug9XlJS0qwXr6ioQHR0NJ5++un7Xvt2Fy9ebHC/zc/Pr1mvS5bDzcEOX0yNxcqDaXjvt/P47XQuzt0oxZKpsegS5C51eUREJIFmhRt3d/1fFu7u7pg2bVqTrzdq1CiMGjWqOSUA0IYZDw+PZj+PLJMgCHi6f1v0CPPAi2tOIu1mJcZ/cQgLHu2CJ3uFQhAEqUskIiIjala4WblyZWvV0Sw9evSAUqlE165dMX/+fPTr10/qksgExIZ54tcX++OVH1Ox52IB5m04jeTrRfj3uK5wljfrjzoREZkxs+pQHBgYiGXLlmH9+vVYv349QkNDkZiYiJSUlHs+R6lUorS0tMFGlsvT2R7fTO+F10ZGwUYmYOPJbIxdchCX88qkLo2IiIzEoPPcPAhBELBx40aMGzeuWc8bOHAgwsLC7jl54Pz587FgwYK79nOeG8t39NpNvPj9SeSXKeFoZ4P3JnTF+JgQqcsiIqIWsKp5buLj43HlypV7Hp83bx4UCoVuy8zMNGJ1JKXe7byx5aUB6BfhjSqVGi//cArzNvyBapXpTjJJREQPzuzDTWpqKgIDA+95XC6Xw83NrcFG1sPXVY5vn+6N2UMiIQjA98mZmPDFIaQVVkhdGhERtRJJe1mWl5c3aHW5fv06UlNT4eXlhbCwMMybNw/Z2dn49ttvAQCLFy9G27Zt0aVLF1RXV+Prr7/G7t27sX37dqneApkBG5mAl4d1QM82npizNhXnckrxyGcH8H+PdcfobvcOxkREZJ4kbbk5fvw4YmJiEBMTAwB45ZVXEBMTgzfffBMAkJOTg4yMDN35NTU1+Nvf/oZu3bph4MCBOHXqFHbu3IkhQ4ZIUj+ZlwGRvtjy0gD0auOJcmUtXlidgvmbz6KmViN1aUREZEAm06HYWLhwJqnUGny4/SK+TLoGAIgO9cCSKTEI8XSSuDIiIroXq+pQTNRcdjYyzBvVCV9P6wl3RzucyizBw58ewK7zeVKXRkREBsBwQ1ZraGd//Ppif0SHuENRpcJf/nsc7/9+AbVq3qYiIjJnDDdk1UK9nPDTc30xo28bAMCypKuY8tVR5CqqpS2MiIhajOGGrJ69rQzzH+2CJVNi4SK3RXJaER7+dD/2Xy6QujQiImoBhhuiOg93D8QvL/ZHp0A33KyowbQVyVi04xLUGqvqc09EZPYYbohu09bHGRtf6IvJ8aEQReCTXZcxfUUyCsuVUpdGRERNxHBDdAcHOxssnNAdH0+KhqOdDQ5cKcToT/Yj+XqR1KUREVETMNwQ3cOE2BD8PKsfIvxckF+mxOSvjmDp3qvQ8DYVEZFJY7gh0qODvyt+ntkP43oEQa0R8cHWC3j22+MoqayRujQiIroHhhui+3CW22LREz2wcEI32NvKsOtCPh7+9ABOZhRLXRoRETWC4YaoCQRBwOT4MGx8oS/CvZ2QXVKFSV8exsqD12FlK5gQEZk8hhuiZugS5I5fXuyPUV0DoFKLWPDLObywOgWl1SqpSyMiojoMN0TN5OZghy+mxuKtMZ1hZyPg9zO5ePSzAzh7QyF1aUREBIYbohYRBAFP9WuLH/9fAoI9HJF2sxLjvziE75MzeJuKiEhiDDdEDyAmzBNbXuqPwVF+qKnVYN6G03juuxPIUVRJXRoRkdViuCF6QB5O9vh6Wk/8Y1QUbGQCtp3Nw9CPkvD1/mtcYZyISAIMN0QGIJMJeG5ge2x5qT96hnuiokaNf285jzGfH0QKh4wTERkVww2RAUUFuOHH/5eADyZ2g4eTHc7nlGLi0kP458bTUFRyRBURkTEw3BAZmEwm4IleYdj1ykA8FhcCUQTWHM3A4I/2YkNKFjscExG1MoYbolbi7SLHh49H44e/9kGknwtuVtTglR9PYfJXR3Alv1zq8oiILBbDDVEr693OG1teGoDXRkbBwU6GI9eKMOqTffhw20VUq9RSl0dEZHEYboiMwN5WhucT22PHywMxJMoPKrWIz/dcwfBF+7D3Yr7U5RERWRSGGyIjCvVywtfTe+LLP8ch0N0BGUWVmLHyGF5YfQK5imqpyyMisggMN0RGJggCRnQJwM5XBuLZAW1hIxPw2+lcDPloL1YcuM65cYiIHpAgWtnQjdLSUri7u0OhUMDNzU3qcohwPqcUr288jZSMEgBA50A3vDehG3qEekhaFxGRKWnO9zdbbogk1inQDeue64uFE7rB3dEO53JKMf6Lg/jXptNQVHFuHCKi5mK4ITIBMpmAyfFh2PW3gZgYq50b57sjGRjy0V5sOpnNuXGIiJqB4YbIhPi4yPHRpGh8/2wftPd1RmF5Deb8kIqpXx/F1QLOjUNE1BQMN0QmKKG9N36f/RBeHdERclsZDl29iVGL9+PjHZc4Nw4R0X0w3BCZKHtbGWYOisCOlwcisaMvatQafLrrMkYs3oekSwVSl0dEZLIYbohMXJi3E1bO6IWlU2MR4OaA9JuVmL4iGTPXpCCvlHPjEBHdieGGyAwIgoBR3QKx828D8Zf+bSETgC1/5GDIR0lYdfA61Bp2OCYiqsd5bojM0JlsBf616QxSM0sAAN2C3fHu+K7oHuIhaV1ERK2F89wQWbiuwe7Y8HxfvDu+K9wcbHE6W4GxSw7izZ/PoLSac+MQkXVjuCEyUzKZgKm9w7Hrb4mYEBMMUQS+PZyOIR8l4edUzo1DRNaL4YbIzPm6yvHxEz2w5pneaOfrjIIyJWavTcWfv0nG9cIKqcsjIjI6hhsiC9E3wge/zx6Avw3rALmtDAeuFGLE4n1YvJNz4xCRdWG4IbIgclsbvDgkEttffggDO/iiplaDxTsvY+Tifdh/mXPjEJF1YLghskDh3s5Y9VQvLJkSC383OdJuVuLP3yTjpe9PIr+Mc+MQkWVjuCGyUIIg4OHugdj5ykA81a8NZAKw+dQNDPkwCd8eTuPcOERksTjPDZGVOJOtwOsbT+NUlgIA0D3EHe+O64ZuIe4SV0ZEdH+c54aI7tI12B0bXuiHd8Z1hauDLf7IUmDskgOYv/ks58YhIovCcENkRWxkAv7cJxy7/jYQY3sEQSMCqw6lYehHSfjl1A3OjUNEFoHhhsgK+bk64JMnY7D6md5o6+OM/DIlXvz+JKatSEYa58YhIjPHcENkxfrVzY3z8tAOsLeVYf/lQgxfvA+f7LwMZS3nxiEi88RwQ2TlHOxsMHtoJLbPeQgDIn1QU6vBop2XMGrxfhy8Uih1eUREzcZwQ0QAgDY+zvj26Xh8NjkGvq5yXCuswNSvj2LO2pMoKFNKXR4RUZMx3BCRjiAIGBMdhF1/G4gZfbVz42xKvYHBH+3F/46kc24cIjILnOeGiO7pj6wSvL7xDE5na+fGiQ71wLvjuqJrMOfGISLj4jw3RGQQ3UM8sGlmP7w9tgtc5bY4lVmCRz8/gAW/nEUZ58YhIhPFcENEetnIBExLaINdfxuIMdHauXFWHkzD0I+T8NvpHM6NQ0Qmh+GGiJrEz80Bn02Owf/+Eo823k7IK1XihdUpmLHyGNJvcm4cIjIdDDdE1CwDIn2xdc5DmD0kEvY2MiRdKsDwRfvw+W7OjUNEpoHhhoiazcHOBi8P64Ctcwagf4QPlLUafLj9EkZ9wrlxiEh6HC1FRA9EFEVsPnUD7/x6HoXl2vlw+rTzwuwhHZDQ3lvi6ojIUjTn+5vhhogMQlGlwqIdl7D6aDpUau0/K73bemH20EgktPOGIAgSV0hE5ozhRg+GG6LWdaOkCkv3XsUPxzJRo9YAAOLbaENO3/YMOUTUMgw3ejDcEBlHjkIbctYm3wo5PcM9MWdoB/SLYMghouZhuNGD4YbIuHIV1ViWdBVrkjNQU6sNOXHhnpg9JBIDIn0YcoioSRhu9GC4IZJGXmldyDmaAWVdyIkJ88CcoR3wEEMOEd0Hw40eDDdE0sovrcaypGtYfTRdF3J6hHpg9tBIJHbwZcghokYx3OjBcENkGvLLqrE86Rq+O5qOapU25ESHuGP20EgM6ujHkENEDTDc6MFwQ2RaCsqUWL7vKv535FbI6R7ijtlDIjE4iiGHiLQYbvRguCEyTYXlSny17xq+PZyOKpV2GYduwe54aUgkhnZiyCGydgw3ejDcEJm2m+VKfLX/Or49nIbKGm3I6RLkhpeGRGJ4Z3+GHCIrxXCjB8MNkXkoqqjBV/uv4dtDaaioCzmdAt0wuy7kyGQMOUTWhOFGD4YbIvNSXFGDrw9cw6qDt0JOVIArZg+JxIguAQw5RFaC4UYPhhsi81RcUYNvDlzHqkNpKFfWAtCGnJeGRGIkQw6RxWO40YPhhsi8lVTWYMWB61h5MA1ldSGno78rXhwSgdFdAxlyiCwUw40eDDdElkFRqcKKg9ex4uB1lFVrQ06knwteHBKJh7sFwoYhh8iiNOf7W2akmhq1b98+jBkzBkFBQRAEAZs2bbrvc/bu3YvY2FjI5XJERERg1apVrV4nEZkedyc7vDysAw68NhhzhkbCzcEWl/PL8dL3JzFi8T78nJoNtcaq/u9GRHUkDTcVFRWIjo7GkiVLmnT+9evX8fDDD2PQoEFITU3FnDlz8Mwzz2Dbtm2tXCkRmSp3RzvMGdoBB/4xGK8M6wA3B1tcyS/H7LWpGL4oCZtOMuQQWRuTuS0lCAI2btyIcePG3fOc1157DVu2bMGZM2d0+5588kmUlJRg69atTXod3pYismyl1Sr892Aavj5wHYoqFQCgnY8zXhwSgTHdg2BrI+n/6YiohczmtlRzHT58GEOHDm2wb8SIETh8+PA9n6NUKlFaWtpgIyLL5eZghxeHROLAa4Pw6oiO8HCyw7XCCrz8wykMW7QP609koVatkbpMImpFZhVucnNz4e/v32Cfv78/SktLUVVV1ehzFi5cCHd3d90WGhpqjFKJSGKuDnaYOSgCB14bjL+P7AhPJztcL6zA3346haEfJ+Gn45kMOUQWyqzCTUvMmzcPCoVCt2VmZkpdEhEZkYvcFi8kakPOayOj4OVsj7SblXh13R8Y/FESfjyeCRVDDpFFMatwExAQgLy8vAb78vLy4ObmBkdHx0afI5fL4ebm1mAjIuvjLLfF84ntsf/vgzBvVBS8ne2RUVSJv6/7A4M/2osfjmUw5BBZCLMKNwkJCdi1a1eDfTt27EBCQoJEFRGRuXGW2+L/DWyP/a8Nwj9HR8HHxR6ZRVV4bf1pDPpwL9YmZ6CmliGHyJxJGm7Ky8uRmpqK1NRUANqh3qmpqcjIyACgvaU0bdo03fnPPfccrl27hr///e+4cOECvvjiC/z44494+eWXpSifiMyYk70t/vpQe+z/+2D86+FO8HGRI6u4Cv/YoA05a44y5BCZK0mHgu/duxeDBg26a//06dOxatUqzJgxA2lpadi7d2+D57z88ss4d+4cQkJC8MYbb2DGjBlNfk0OBSeixlTVqLEmOQPLkq6ioEwJAAj2cMTzie3xeM8QyG1tJK6QyLpx+QU9GG6ISJ9qlRprjmpDTn5dyAlyd8DzgyIwiSGHSDIMN3ow3BBRU1Sr1FibnIGlSVeRV6oNOYHuDng+sT0m9QyFgx1DDpExMdzowXBDRM1RrVLjh2OZWLr3KnJLqwEAAW4OeG5gOzwZH8aQQ2QkDDd6MNwQUUsoa9X48Vgmvth7FTkKbcjxd5PjuYHtMZkhh6jVMdzowXBDRA9CWavGT8ez8MWeK7hRF3J8XbUhZ2pvhhyi1sJwowfDDREZgrJWjXUnsvDFnqvILtEu/+LjIsdzA9thUq9QuDnYSVwhkWVhuNGD4YaIDKmmVoP1KVn4fPcVXciR28owvEsAJsYGY0CkL2xkgsRVEpk/hhs9GG6IqDXU1GqwISUL3xy4jsv55br9fq5yjI8JxsS4EHTwd5WwQiLzxnCjB8MNEbUmURRxOluB9SeysPnUDRRXqnTHugW7Y2JsMB7tEQwvZ3sJqyQyPww3ejDcEJGx1NRqsOdiPtafyMLuC/mo1Wj/ubWVCRgU5YeJsSEYHOUHe1uzWuaPSBIMN3ow3BCRFIoqarA5NRvrU7JxOluh2+/pZIdHo4MwMS4E3YLdIQjsn0PUGIYbPRhuiEhql/LKsD4lC5tOZutmPwaASD8XTIgNwfiYYAS4O0hYIZHpYbjRg+GGiEyFWiPiwJVCrD+RhW1nc6GsW4VcJgD9InzwWFwIhncOgKM9584hYrjRg+GGiExRabUKv/2Rg/UpWTiWVqzb7yK3xcPdAjExLgS92njythVZLYYbPRhuiMjUpd+swIaUbGw4mYXMoird/lAvR0yICcHE2BCEeTtJWCGR8THc6MFwQ0TmQqMRcSytCOtTsvDb6VyUK2t1x+LbeGFiXDBGdwuEK2dDJivAcKMHww0RmaOqGjW2nc3F+pQsHLhSiPp/uR3sZBjRJQATYkPQP8KHsyGTxWK40YPhhojMXY6iChtPZmP9iSxcLajQ7fd3k2NcTDAeiw1BJGdDJgvDcKMHww0RWQpRFPFHlgLrU7SzIZfcNhty9xB3TIwNwaPRQfDkbMhkARhu9GC4ISJLpKxVY8+FfKw7kY29F2/NhmxnI2Bw3WzIiR05GzKZL4YbPRhuiMjSFZYrsTn1BtanZOHsjVLdfi9ne+1syLEh6BrsxmHlZFYYbvRguCEia3IhtxQbUrKx8WQ2CspuzYbcwd8FE2NDMC4mGP5unA2ZTB/DjR4MN0RkjWrVGuyvmw15+7k81Nw2G3L/SF9MjA3GiC4BcLDjbMhkmhhu9GC4ISJrp6hSYUvdbMgn0m/Nhuwqt8XD3bWzIfcM52zIZFoYbvRguCEiuiWtsAIbUrKwPiUb2SW3ZkMO93bChJgQTIgNRqgXZ0Mm6THc6MFwQ0R0N41GxNHr2tmQfz+dg4oate5Y77ZemBgbgtHdA+Eit5WwSrJmDDd6MNwQEelXWVOLrWe0syEfunqzwWzII7sEYGJcCPq252zIZFwMN3ow3BARNd2NkrrZkFOycO222ZAD3BwwPjYYE2ODEeHH2ZCp9THc6MFwQ0TUfKIoIjWzBOtTsvDLqRwoqm7Nhhwd4o6JcSEY052zIVPrYbjRg+GGiOjBKGvV2HU+H+tPZGHvpQKob5sNeUiUPx7tEYQBkT5crZwMiuFGD4YbIiLDKShT4ufUbGxIyca5nFuzIdvKBPRq44XBUX4YFOWH9r7OHFpOD4ThRg+GGyKi1nE+pxQbT2Zj57k8XCusaHAszMsJgzr6YlCUH/q08+ZkgdRsDDd6MNwQEbW+tMIK7L6Qjz0X83H0WhFq1BrdMQc7Gfq198GgKD8MjvJDkIejhJWSuWC40YPhhojIuCqUtTh4pRB7LuZjz4UC5JZWNzgeFeCKQVF+GNTRD7FhHrC14crldDeGGz0YboiIpCOKIs7llGLvxQLsvpCPkxnF0Nz2LeTuaIeHOvhicJQvBnbwgxdHX1Edhhs9GG6IiExHcUUNki4VYM/FfCRdKkBJ5a0h5oIA9Aj1wOCO2k7JXYLc2CnZijHc6MFwQ0RkmmrVGqRmlmD3hXzsvpCPC7llDY77ucoxqC7o9I/04VIQVobhRg+GGyIi85CjqMKeC9rbVwevFKJKdWu9KzsbAb3beiOxoy8GR/mhna+LhJWSMTDc6MFwQ0RkfqpVahy9XoQ9dSOw0m9WNjjexttJN/oqvq0X5LYcam5pGG70YLghIjJvoijiWmGFLugkXy+CSn3rq8zJ3gb9Iny0Ewh29EOAu4OE1ZKhMNzowXBDRGRZyqpVOHilsG5enQIUlCkbHO8U6IbBUdrbVz1CPbmauZliuNGD4YaIyHJpNNqh5vWdkk9lleD2bzlPJzsM7KCdKXlgB194OHGoublguNGD4YaIyHrcLFci6ZK2U/K+SwUora7VHZMJQGyYp66vTlSAK4eamzCGGz0YboiIrFOtWoMT6cXYfTEfey8U4GJew6Hmge4OSOyoDTr9IrzhZM+h5qaE4UYPhhsiIgKArOJK7LlYgD0X8nHoaiGqVbfWv7K3laFPO28MqhtqHu7tLGGlBDDc6MVwQ0REd6pWqXH42k3sqeurk1Vc1eB4O19n3UzJvdp4wd6W618ZG8ONHgw3RESkjyiKuJJfjj0XtUHneFoxam9bAMtFbov+dUPNEzv6ws+NQ82NgeFGD4YbIiJqjtJqFfZf0q5qvvdiPgrLaxoc7xrspmvViQ7xgIxDzVsFw40eDDdERNRSGo2I09mKujl18vFHlqLBcW9nezzUwRe92nghNtwDkX6unFfHQBhu9GC4ISIiQykoU2LvRW3Q2X+pEGXK2gbHXeW26BHmgdgwT8SFe6JHmAfcHOwkqta8MdzowXBDREStQaXW4HhaMQ5eKURKRjFSM0tQWaNucI4gAB39XRFTF3biwj3RxtuJ8+s0AcONHgw3RERkDLVqDS7kluFkRjFOpBfjREYxMouq7jrPy9kesWEeiA33RFyYJ7qHeMDRngt/3onhRg+GGyIikkp+WTVS0kuQklGMlPRi/JGtQE2tpsE5tjIBnYPcEBvmqQ084Z4Icnew+tYdhhs9GG6IiMhUKGvVOHujFCnpxUipa+HJK1XedV6AmwPiwj0RE+aBuHBPdAlyt7q5dhhu9GC4ISIiUyWKIrJLqpCSUYKUdG3YOZdTCrWm4Ve1va0M3YPdEReubd2JDfOEr6tcoqqNg+FGD4YbIiIyJ5U1tfgjS4ET6cW6/jvFlaq7zgvzcrot7Higo78rbG0sp3WH4UYPhhsiIjJnoijiemEFUjJKcCJd23fnUn4Z7vw2d7a30Q1Djw33RGyoJ9ydzHcYOsONHgw3RERkaUqrVUitDzsZxTiZUYLyO+bcAYBIPxfdnDux4Z5o5+NsNjMqM9zowXBDRESWTq0RcTm/TDsEPV0bdq4XVtx1nrujHWLrOinHhnkiOtQDznJbCSq+P4YbPRhuiIjIGt0sV2o7Ktf12zmVWQLlHcPQZQLQKdBNF3biwj0R4uloEsPQGW70YLghIiLSzqh8Pqe0QetOdsndkwz6usoRF+aJ2PBbw9Ad7Iw/ySDDjR4MN0RERI3LUVQhJf1W352zNxRQqe8Yhm4jQ5dgN8Td1nfH382h1WtjuNGD4YaIiKhpqlVqnM5W6ObcSckoRmF5zV3nBXs46tbKig3zRFSgK+wMPAyd4UYPhhsiIqKWEUURmUVVOJFRVDcMvQQXcktxxxyDaOvjjD1zEw362s35/jbNLtFERERkcgRBQJi3E8K8nTA+JgQAUK6sxanMW7eyUtKL0SnQVdI6GW6IiIioxVzktugX4YN+ET4AAI1GRFkjc+wYk+XMy0xERESSk8kEuDtKOxMyww0RERFZFIYbIiIisigMN0RERGRRTCLcLFmyBG3atIGDgwN69+6N5OTke567atUqCILQYHNwaP3Jg4iIiMg8SB5ufvjhB7zyyit46623kJKSgujoaIwYMQL5+fn3fI6bmxtycnJ0W3p6uhErJiIiIlMmebj5+OOP8eyzz+Kpp55C586dsWzZMjg5OWHFihX3fI4gCAgICNBt/v7+RqyYiIiITJmk4aampgYnTpzA0KFDdftkMhmGDh2Kw4cP3/N55eXlCA8PR2hoKMaOHYuzZ8/e81ylUonS0tIGGxEREVkuScNNYWEh1Gr1XS0v/v7+yM3NbfQ5HTt2xIoVK/Dzzz/ju+++g0ajQd++fZGVldXo+QsXLoS7u7tuCw0NNfj7ICIiItMh+W2p5kpISMC0adPQo0cPDBw4EBs2bICvry++/PLLRs+fN28eFAqFbsvMzDRyxURERGRMki6/4OPjAxsbG+Tl5TXYn5eXh4CAgCZdw87ODjExMbhy5Uqjx+VyOeRy+QPXSkREROZB0pYbe3t7xMXFYdeuXbp9Go0Gu3btQkJCQpOuoVarcfr0aQQGBrZWmURERGRGJF8485VXXsH06dPRs2dPxMfHY/HixaioqMBTTz0FAJg2bRqCg4OxcOFCAMDbb7+NPn36ICIiAiUlJfjPf/6D9PR0PPPMM1K+DSIiIjIRkoebJ554AgUFBXjzzTeRm5uLHj16YOvWrbpOxhkZGZDJbjUwFRcX49lnn0Vubi48PT0RFxeHQ4cOoXPnzlK9BSIiIjIhgiiKotRFGJNCoYCHhwcyMzPh5uYmdTlERETUBKWlpQgNDUVJSQnc3d31nit5y42xlZWVAQCHhBMREZmhsrKy+4Ybq2u50Wg0uHHjBlxdXSEIgkGvXZ8qrbVVyNrfP8DPgO/fut8/wM/A2t8/0HqfgSiKKCsrQ1BQUIPuKo2xupYbmUyGkJCQVn0NNzc3q/1DDfD9A/wM+P6t+/0D/Ays/f0DrfMZ3K/Fpp7ZTeJHREREpA/DDREREVkUhhsDksvleOutt6x2RmRrf/8APwO+f+t+/wA/A2t//4BpfAZW16GYiIiILBtbboiIiMiiMNwQERGRRWG4ISIiIovCcENEREQWheHGQJYsWYI2bdrAwcEBvXv3RnJystQlGc2+ffswZswYBAUFQRAEbNq0SeqSjGrhwoXo1asXXF1d4efnh3HjxuHixYtSl2VUS5cuRffu3XWTdiUkJOD333+XuizJvP/++xAEAXPmzJG6FKOZP38+BEFosEVFRUldllFlZ2fjT3/6E7y9veHo6Ihu3brh+PHjUpdlFG3atLnr918QBMycOVOSehhuDOCHH37AK6+8grfeegspKSmIjo7GiBEjkJ+fL3VpRlFRUYHo6GgsWbJE6lIkkZSUhJkzZ+LIkSPYsWMHVCoVhg8fjoqKCqlLM5qQkBC8//77OHHiBI4fP47Bgwdj7NixOHv2rNSlGd2xY8fw5Zdfonv37lKXYnRdunRBTk6Objtw4IDUJRlNcXEx+vXrBzs7O/z+++84d+4cPvroI3h6ekpdmlEcO3aswe/9jh07AACPP/64NAWJ9MDi4+PFmTNn6h6r1WoxKChIXLhwoYRVSQOAuHHjRqnLkFR+fr4IQExKSpK6FEl5enqKX3/9tdRlGFVZWZkYGRkp7tixQxw4cKA4e/ZsqUsymrfeekuMjo6WugzJvPbaa2L//v2lLsNkzJ49W2zfvr2o0WgkeX223DygmpoanDhxAkOHDtXtk8lkGDp0KA4fPixhZSQVhUIBAPDy8pK4Emmo1WqsXbsWFRUVSEhIkLoco5o5cyYefvjhBv8eWJPLly8jKCgI7dq1w9SpU5GRkSF1SUazefNm9OzZE48//jj8/PwQExODr776SuqyJFFTU4PvvvsOTz/9tMEXqG4qhpsHVFhYCLVaDX9//wb7/f39kZubK1FVJBWNRoM5c+agX79+6Nq1q9TlGNXp06fh4uICuVyO5557Dhs3bkTnzp2lLsto1q5di5SUFCxcuFDqUiTRu3dvrFq1Clu3bsXSpUtx/fp1DBgwAGVlZVKXZhTXrl3D0qVLERkZiW3btuH555/HSy+9hP/+979Sl2Z0mzZtQklJCWbMmCFZDVa3KjhRa5o5cybOnDljVX0N6nXs2BGpqalQKBRYt24dpk+fjqSkJKsIOJmZmZg9ezZ27NgBBwcHqcuRxKhRo3Q/d+/eHb1790Z4eDh+/PFH/OUvf5GwMuPQaDTo2bMn3nvvPQBATEwMzpw5g2XLlmH69OkSV2dc33zzDUaNGoWgoCDJamDLzQPy8fGBjY0N8vLyGuzPy8tDQECARFWRFGbNmoVff/0Ve/bsQUhIiNTlGJ29vT0iIiIQFxeHhQsXIjo6Gp988onUZRnFiRMnkJ+fj9jYWNja2sLW1hZJSUn49NNPYWtrC7VaLXWJRufh4YEOHTrgypUrUpdiFIGBgXcF+U6dOlnVrTkASE9Px86dO/HMM89IWgfDzQOyt7dHXFwcdu3apdun0Wiwa9cuq+tvYK1EUcSsWbOwceNG7N69G23btpW6JJOg0WigVCqlLsMohgwZgtOnTyM1NVW39ezZE1OnTkVqaipsbGykLtHoysvLcfXqVQQGBkpdilH069fvrikgLl26hPDwcIkqksbKlSvh5+eHhx9+WNI6eFvKAF555RVMnz4dPXv2RHx8PBYvXoyKigo89dRTUpdmFOXl5Q3+d3b9+nWkpqbCy8sLYWFhElZmHDNnzsSaNWvw888/w9XVVdfXyt3dHY6OjhJXZxzz5s3DqFGjEBYWhrKyMqxZswZ79+7Ftm3bpC7NKFxdXe/qY+Xs7Axvb2+r6Xs1d+5cjBkzBuHh4bhx4wbeeust2NjYYPLkyVKXZhQvv/wy+vbti/feew+TJk1CcnIyli9fjuXLl0tdmtFoNBqsXLkS06dPh62txPFCkjFaFuizzz4Tw8LCRHt7ezE+Pl48cuSI1CUZzZ49e0QAd23Tp0+XujSjaOy9AxBXrlwpdWlG8/TTT4vh4eGivb296OvrKw4ZMkTcvn271GVJytqGgj/xxBNiYGCgaG9vLwYHB4tPPPGEeOXKFanLMqpffvlF7Nq1qyiXy8WoqChx+fLlUpdkVNu2bRMBiBcvXpS6FFEQRVGUJlYRERERGR773BAREZFFYbghIiIii8JwQ0RERBaF4YaIiIgsCsMNERERWRSGGyIiIrIoDDdERERkURhuiMjqCYKATZs2SV0GERkIww0RSWrGjBkQBOGubeTIkVKXRkRmimtLEZHkRo4ciZUrVzbYJ5fLJaqGiMwdW26ISHJyuRwBAQENNk9PTwDaW0ZLly7FqFGj4OjoiHbt2mHdunUNnn/69GkMHjwYjo6O8Pb2xl//+leUl5c3OGfFihXo0qUL5HI5AgMDMWvWrAbHCwsLMX78eDg5OSEyMhKbN29u3TdNRK2G4YaITN4bb7yBiRMn4tSpU5g6dSqefPJJnD9/HgBQUVGBESNGwNPTE8eOHcNPP/2EnTt3NggvS5cuxcyZM/HXv/4Vp0+fxubNmxEREdHgNRYsWIBJkybhjz/+wOjRozF16lQUFRUZ9X0SkYFIvXInEVm36dOnizY2NqKzs3OD7d133xVFUbvq+nPPPdfgOb179xaff/55URRFcfny5aKnp6dYXl6uO75lyxZRJpOJubm5oiiKYlBQkPj666/fswYA4r/+9S/d4/LychGA+PvvvxvsfRKR8bDPDRFJbtCgQVi6dGmDfV5eXrqfExISGhxLSEhAamoqAOD8+fOIjo6Gs7Oz7ni/fv2g0Whw8eJFCIKAGzduYMiQIXpr6N69u+5nZ2dnuLm5IT8/v6VviYgkxHBDRJJzdna+6zaRoTg6OjbpPDs7uwaPBUGARqNpjZKIqJWxzw0RmbwjR47c9bhTp04AgE6dOuHUqVOoqKjQHT948CBkMhk6duwIV1dXtGnTBrt27TJqzUQkHbbcEJHklEolcnNzG+yztbWFj48PAOCnn35Cz5490b9/f6xevRrJycn45ptvAABTp07FW2+9henTp2P+/PkoKCjAiy++iD//+c/w9/cHAMyfPx/PPfcc/Pz8MGrUKJSVleHgwYN48cUXjftGicgoGG6ISHJbt25FYGBgg30dO3bEhQsXAGhHMq1duxYvvPACAgMD8f3336Nz584AACcnJ2zbtg2zZ89Gr1694OTkhIkTJ+Ljjz/WXWv69Omorq7GokWLMHfuXPj4+OCxxx4z3hskIqMSRFEUpS6CiOheBEHAxo0bMW7cOKlLISIzwT43REREZFEYboiIiMiisM8NEZk03jknouZiyw0RERFZFIYbIiIisigMN0RERGRRGG6IiIjIojDcEBERkUVhuCEiIiKLwnBDREREFoXhhoiIiCwKww0RERFZlP8P1xdSkOVob98AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10                              # 訓練次數\n",
    "early_stopping = 3                      # 模型訓練幾次沒進步就停止\n",
    "stop_cnt = 0                             # 計數模型是否有進步的計數器\n",
    "model_path = 'model.ckpt'                # 模型存放路徑\n",
    "show_loss = True                         # 是否顯示訓練折線圖\n",
    "best_loss = float('inf')                             # 最佳的準確率\n",
    "loss_record = {'train':[], 'valid':[]}   # 訓練紀錄\n",
    "\n",
    "for epoch in range(epochs):   \n",
    "    train_loss = train(epoch)\n",
    "    valid_loss = valid(epoch)\n",
    "    \n",
    "    loss_record['train'].append(train_loss)\n",
    "    loss_record['valid'].append(valid_loss)\n",
    "    \n",
    "    # 儲存最佳的模型權重\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f'Saving Model With Loss {valid_loss:.5f}')\n",
    "        stop_cnt = 0\n",
    "    else:\n",
    "        stop_cnt+=1\n",
    "    \n",
    "    # Early stopping\n",
    "    if stop_cnt == early_stopping:\n",
    "        output = \"Model can't improve, stop training\"\n",
    "        print('-' * (len(output)+2))\n",
    "        print(f'|{output}|')\n",
    "        print('-' * (len(output)+2))\n",
    "        break\n",
    "\n",
    "    print(f'Train Loss: {train_loss:.5f} Valid Loss: {valid_loss:.5f}', end='| ')\n",
    "    print(f'Best Loss: {best_loss:.5f}', end='\\n\\n')\n",
    "\n",
    "if show_loss:\n",
    "    show_training_loss(loss_record)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1335fbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b01d1653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文章:\n",
      " summarize: jio wednesday announced launch 25000kilometre asiaafricaeurope aae1 submarine cable system . jio claimed longest technologybased 100 gbps gigabits per second submarine cable system world stretching france hong kong 21 cable landings across asia europe . project combined work leading service providers various countries .\n",
      "原始摘要:\n",
      " jio launches worlds longest 100gbps underwater cable system\n",
      "生成摘要:\n",
      " jio wednesday announced launch 25000kilometre asiaafricaeurope aae1 submarine cable system\n"
     ]
    }
   ],
   "source": [
    "data_idx = 5000\n",
    "input_ids = tokenizer(x_valid[data_idx], return_tensors='pt').input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, num_beams=2, max_length=30, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
    "preds = [tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=True) for i in generated_ids][0]\n",
    "print('原始文章:\\n', x_valid[data_idx])\n",
    "print('原始摘要:\\n', y_valid[data_idx])\n",
    "print('生成摘要:\\n', preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9db2a5e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12700\\1653364591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrouge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "score = rouge.get_scores(y_valid[data_idx],preds)[0]\n",
    "for i in score[0]:\n",
    "    print(score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a80330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
